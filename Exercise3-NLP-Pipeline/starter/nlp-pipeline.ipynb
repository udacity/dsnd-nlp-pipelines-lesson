{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: NLP Pipeline with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first want to make sure spaCy is ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also read in some data into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/reviews.csv')\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing features (`X`) & target (`y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "\n",
    "# separate features from labels\n",
    "X = data.drop('recommend', axis=1)\n",
    "y = data['recommend'].copy()\n",
    "\n",
    "print('Labels:', y.unique())\n",
    "print('Features:')\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split the data into a train & test sets so we can evaluate our\n",
    "end model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    random_state=27,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Pipeline: Splitting Numerical, Categorical, and Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to separate the data into the different feature types so we can better\n",
    "process & utilize them as features for our model.\n",
    "Depending on the situation, you may instead only use certain features & feature\n",
    "types or even do more feature engineering by combining the given data columns!\n",
    "\n",
    "However, in this scenario, we're going to have you define the following feature\n",
    "groups:\n",
    "\n",
    "- Numerical: `num_features`\n",
    "- Categorical: `cat_features`\n",
    "- Text: `text_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be useful to use in creating a pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split data into numerical, categorical, and text features\n",
    "\n",
    "num_features = (\n",
    "    # TODO: your code here\n",
    ")\n",
    "print('Numerical features:', num_features)\n",
    "\n",
    "cat_features = (\n",
    "    # TODO: your code here\n",
    ")\n",
    "print('Categorical features:', cat_features)\n",
    "\n",
    "\n",
    "text_features = (\n",
    "    # TODO: your code here\n",
    ")\n",
    "print ('Review Text features:', text_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define pipeline for numerical features called `num_pipeline``\n",
    "\n",
    "num_pipeline = None\n",
    "\n",
    "num_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define pipeline for categorical features called `cat_pipeline`\n",
    "cat_pipeline = None\n",
    "\n",
    "cat_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the text part of the pipeline, there are multiple ways we can process the\n",
    "pipeline.\n",
    "\n",
    "We specifically are going to utilize spaCy and some built-in Python functions to\n",
    "process the text in our custom Scikit-learn `Transformers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom `Transformer`: Count Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create a `CountCharacter()` Scikit-learn `Transformer` using \n",
    "[`BaseEstimator`](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html) and\n",
    "[`TransformerMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html#sklearn.base.TransformerMixin).\n",
    "\n",
    "This custom `Transformer` will take in a string for a character to return the\n",
    "number of times a certain character appears in the text input.\n",
    "This way we have a way to see how many times a certain character\n",
    "(like an exclamation point `!`)\n",
    "appears.\n",
    "You can use built-in Python functions to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# TODO: create CountCharacter()\n",
    "# Takes in a string for the character to count\n",
    "# Outputs the number times that character appears in the text\n",
    "\n",
    "class CountCharacter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, character: str):\n",
    "        self.character = character\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # TODO: your code here\n",
    "        return [[]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use `CountCharacter()` to create a feature for the following:\n",
    "\n",
    "- Number of spaces in the text\n",
    "- Number of exclamations (`!`) in the text\n",
    "- Number of question marks (`?`) in the text\n",
    "\n",
    "You may find using [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html)\n",
    "to be useful in your pipeline.\n",
    "\n",
    "> Note:\n",
    "> We also provided an `initial_text_preprocess` to make sure the text is in the\n",
    "> expected shape for your `CountCharacter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "initial_text_preprocess = Pipeline([\n",
    "    (\n",
    "        'dimension_reshaper',\n",
    "        FunctionTransformer(\n",
    "            np.reshape,\n",
    "            kw_args={'newshape':-1},\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "\n",
    "# TODO: create a pipeline for counting the number of spaces, `!`, and `?`\n",
    "feature_engineering = FeatureUnion([\n",
    "    ('count_spaces', CountCharacter(character=' ')),\n",
    "    ('count_exclamations', CountCharacter(character='!')),\n",
    "    ('count_question_marks', CountCharacter(character='?')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should work when the above is complete\n",
    "character_counts_pipeline = Pipeline([\n",
    "    (\n",
    "        'initial_text_preprocess',\n",
    "        initial_text_preprocess,\n",
    "    ),\n",
    "    (\n",
    "        'feature_engineering',\n",
    "        feature_engineering,\n",
    "    ),\n",
    "])\n",
    "character_counts_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom `Transformer`: spaCy and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to use TF-IDF to get a vector representation of the review text.\n",
    "\n",
    "But before we use TF-IDF, we can simplify the text with lemmatization. This way\n",
    "words like 'good' and 'better' are converted to the same value. This\n",
    "representation will carry over into TF-IDF.\n",
    "\n",
    "Create a custom `Transformer` called `SpacyLemmatizer()` to lemmatize the text\n",
    "given.\n",
    "Then in your `tfidf_pipeline`, use `SpacyLemmatizer()` followed by\n",
    "a [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "in your pipeline\n",
    "\n",
    "> Note:\n",
    "> As before, we provided an `initial_text_preprocess` to ensure the text is\n",
    "> in te expected shape for your `SpacyLemmatizer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your SpacyLemmatizer\n",
    "class SpacyLemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "\n",
    "    # TODO: your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_pipeline = Pipeline([\n",
    "    (\n",
    "        'dimension_reshaper',\n",
    "        FunctionTransformer(\n",
    "            np.reshape,\n",
    "            kw_args={'newshape':-1},\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        'lemmatizer',\n",
    "        SpacyLemmatizer(nlp=nlp),\n",
    "    ),\n",
    "    (\n",
    "        'tfidf_vectorizer',\n",
    "        TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "        ),\n",
    "    ),\n",
    "])\n",
    "tfidf_pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Feature Engineering Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "feature_engineering = ColumnTransformer([\n",
    "        ('num', num_pipeline, num_features),\n",
    "        ('cat', cat_pipeline, cat_features),\n",
    "        ('character_counts', character_counts_pipeline, text_features),\n",
    "        ('tfidf_text', tfidf_pipeline, text_features),\n",
    "])\n",
    "\n",
    "feature_engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the feature engineering pipeline created, we will append a\n",
    "machine learning model (a classifier) to be trained with the features\n",
    "engineering pipeline you created.\n",
    "\n",
    "We specifically will use a\n",
    "[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "but in practice, you could use a different kind of model with the features\n",
    "you've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model_pipeline = make_pipeline(\n",
    "    feature_engineering,\n",
    "    RandomForestClassifier(random_state=27),\n",
    ")\n",
    "\n",
    "model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your model has been fitted, let's observe the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_forest_pipeline = model_pipeline.predict(X_test)\n",
    "accuracy_forest_pipeline = accuracy_score(y_test, y_pred_forest_pipeline)\n",
    "\n",
    "print('Accuracy:', accuracy_forest_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use a parameter search to better adjust our model.\n",
    "\n",
    "Using either \n",
    "[`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "or\n",
    "[`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "allows us to use cross-validation (CV) to better evaluate different models\n",
    "independent of the test set.\n",
    "\n",
    "After finding the best parameters based on our search, we can use this\n",
    "fine-tuned model against the test set to observe its performance.\n",
    "\n",
    "----\n",
    "\n",
    "Note that parameter searches can take a significant amount of time. We recommend\n",
    "using `RandomizedSearchCV` since this allows you to specify a number of\n",
    "iterations over a set of parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# TODO: set parameters to randomly search over\n",
    "# A couple parameters with 2-5 options each is plenty\n",
    "my_distributions = dict(\n",
    "    # TODO: your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search = RandomizedSearchCV(\n",
    "    estimator=model_pipeline,\n",
    "    param_distributions=my_distributions,\n",
    "    n_iter=6,     # Try 6 different combinations of parameters\n",
    "    cv=5,         # Use 5-fold cross-validation\n",
    "    n_jobs=-1,    # Use all available processors (for multiprocessing)\n",
    "    refit=True,   # Refit the model using the best parameters found\n",
    "    verbose=3,    # Output of parameters, score, time\n",
    "    random_state=27,\n",
    ")\n",
    "\n",
    "param_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best parameters\n",
    "param_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = param_search.best_estimator_\n",
    "model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_forest_pipeline = model_best.predict(X_test)\n",
    "accuracy_forest_pipeline = accuracy_score(y_test, y_pred_forest_pipeline)\n",
    "\n",
    "print('Accuracy:', accuracy_forest_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-dsnd-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
