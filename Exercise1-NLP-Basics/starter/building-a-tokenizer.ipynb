{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Building a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll code your own tokenizer from scratching using base\n",
    "Python!\n",
    "\n",
    "You might normally start with a pretrained tokenizer, but this exercise will\n",
    "help you get to know see some of the tokenization steps better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = (\n",
    "    'The first time you see The Second Renaissance it may look boring. '\n",
    "    'Look at it at least twice and definitely watch part 2. '\n",
    "    'It will change your view of the matrix. '\n",
    "    'Are the human people the ones who started the war? Is AI a bad thing ?'\n",
    ")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This step is where you'll normalize your text by converting to lowercase,\n",
    "removing punctuation, removing accented characters, etc.\n",
    "\n",
    "For example, the text:\n",
    "```\n",
    "Did Uncle Max like the jalapeño dip?\n",
    "```\n",
    "might be normalized to:\n",
    "```\n",
    "did uncle max like the jalapeno dip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # TODO: Normalize incoming text; can be multiple actions\n",
    "    normalized_text = ''\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out your normalization\n",
    "text_cleaned = normalize_text(sample_text)\n",
    "text_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words that you define yourself.\n",
    "\n",
    "Usually this is already precomputed from prior work, but here you'll decide\n",
    "what counts as a stop word.\n",
    "\n",
    "If you're not sure what stop words to use, you can use this list of words:\n",
    "- of\n",
    "- the\n",
    "- a\n",
    "- an\n",
    "- at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(\n",
    "    text: str,\n",
    "    stop_words: list[str] = list(),\n",
    ") -> str:\n",
    "    # TODO: Remove stop words from incoming text\n",
    "    text_no_stop_words = ''\n",
    "    return text_no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out your stop word removal step (after previous step)\n",
    "text_cleaned = normalize_text(sample_text)\n",
    "text_cleaned = remove_stop_words(text_cleaned)\n",
    "text_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This step will take in the cleaned & normalized text and pretokenize the text\n",
    "into a list of smaller pieces.\n",
    "\n",
    "For example, the text:\n",
    "```\n",
    "Did Uncle Max like the jalapeño dip?\n",
    "```\n",
    "might be normalized & then pretokenized to:\n",
    "```\n",
    "[\n",
    "    'did',\n",
    "    'uncle',\n",
    "    'max',\n",
    "    'like',\n",
    "    'jalapeno',\n",
    "    'dip',\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> list[str]:\n",
    "    # TODO: Pretokenize normalized text\n",
    "    smaller_pieces = list()\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out your pretokenization step (after previous steps)\n",
    "text_cleaned = normalize_text(sample_text)\n",
    "text_cleaned = remove_stop_words(text_cleaned)\n",
    "pretokenized_text = pretokenize_text(text_cleaned)\n",
    "pretokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Tokens\n",
    "\n",
    "This step will take in the list of pretokenized pieces (after the previous \n",
    "preprocessing steps applied to the text) into the tokens that will be used.\n",
    "\n",
    "For example, the text:\n",
    "```\n",
    "Did Uncle Max like the jalapeño dip?\n",
    "```\n",
    "might be normalized, pretokenized, and then tokenized to:\n",
    "```\n",
    "[\n",
    "    'did',\n",
    "    'uncle',\n",
    "    'max',\n",
    "    'like',\n",
    "    'jalapeno',\n",
    "    'dip'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps before breaking things further\n",
    "def tokenize_text(text: str) -> list[str]:\n",
    "    # Apply normalization, stop word removal & pretokenization steps \n",
    "    text_normalized: str = normalize_text(text)\n",
    "    text_no_stop_words: str = remove_stop_words(text_normalized)\n",
    "    pretokenized_text: list[str] = pretokenize_text(text_no_stop_words)\n",
    "    # TODO: Go through pretokenized text to create a list of tokens\n",
    "    tokens = list()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text(sample_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-dsnd-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
