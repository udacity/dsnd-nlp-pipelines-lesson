{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Building a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll code your own tokenizer from scratching using base\n",
    "Python!\n",
    "\n",
    "You might normally start with a pretrained tokenizer, but this exercise will\n",
    "help you get to know see some of the tokenization steps better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = 'Did Uncle Max like the jalapeño dip?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This step is where you'll normalize your text by converting to lowercase,\n",
    "removing punctuation, removing accented characters, etc.\n",
    "\n",
    "For example, the text:\n",
    "\n",
    "```python\n",
    "'Did Uncle Max like the jalapeño dip?'\n",
    "```\n",
    "\n",
    "might be normalized to:\n",
    "\n",
    "```python\n",
    "'did uncle max like the jalapeno dip'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    # COMPLETE: Normalize incoming text; can be multiple actions\n",
    "    # Only keep ASCII letters, numbers, and whitespace characters\n",
    "    acceptable_characters = (\n",
    "        string.ascii_letters\n",
    "        + string.digits\n",
    "        + string.whitespace\n",
    "    )\n",
    "    normalized_text = ''.join(\n",
    "        filter(lambda letter: letter in acceptable_characters, text)\n",
    "    )\n",
    "    # Make text lower-case\n",
    "    normalized_text = normalized_text.lower()\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out your normalization\n",
    "text_cleaned = normalize_text(example_text)\n",
    "text_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This step will text and pretokenize the text into a list of smaller pieces\n",
    "that will form the base of the tokens.\n",
    "\n",
    "For example, the text:\n",
    "\n",
    "```python\n",
    "'Did Uncle Max like the jalapeño dip?'\n",
    "```\n",
    "\n",
    "might be normalized & then pretokenized to:\n",
    "\n",
    "```python\n",
    "[\n",
    "    'Did',\n",
    "    'Uncle',\n",
    "    'Max',\n",
    "    'like',\n",
    "    'the',\n",
    "    'jalapeño',\n",
    "    'dip?',\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> list[str]:\n",
    "    # COMPLETE: Pretokenize normalized text\n",
    "    # Split based on spaces\n",
    "    smaller_pieces = text.split(sep=' ')\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out your pretokenization step\n",
    "pretokenized_text = pretokenize_text(example_text)\n",
    "pretokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words that you define yourself.\n",
    "\n",
    "Usually this is already precomputed from prior work, but here you'll decide\n",
    "what counts as a stop word.\n",
    "\n",
    "If you're not sure what stop words to use, you can use this list of words:\n",
    "- of\n",
    "- the\n",
    "- a\n",
    "- an\n",
    "- at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(\n",
    "    tokens: list[str],\n",
    "    stop_words: list[str] = list(),\n",
    ") -> str:\n",
    "    # TODO: Remove stop words from incoming text\n",
    "    text_no_stop_words = [\n",
    "        token\n",
    "        for token in tokens\n",
    "        if token not in stop_words\n",
    "    ]\n",
    "    return text_no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out your stop word removal step (after previous step)\n",
    "example_text_as_tokens = [\n",
    "    'Did',\n",
    "    'Uncle',\n",
    "    'Max',\n",
    "    'like',\n",
    "    'jalapeño',\n",
    "    'dip?',\n",
    "]\n",
    "\n",
    "my_stop_words = [\n",
    "    'of',\n",
    "    'the',\n",
    "    'a',\n",
    "    'an',\n",
    "    'at',\n",
    "]\n",
    "tokens_filtered = remove_stop_words(\n",
    "    example_text_as_tokens,\n",
    "    stop_words=my_stop_words,\n",
    ")\n",
    "tokens_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalize Tokens\n",
    "\n",
    "This step will combine all the previous steps to create the finals tokens\n",
    "that will be used.\n",
    "\n",
    "For example, the text:\n",
    "```python\n",
    "'Did Uncle Max like the jalapeño dip?'\n",
    "```\n",
    "\n",
    "might be normalized, pretokenized, and stop words removed to form the final\n",
    "tokens:\n",
    "\n",
    "```python\n",
    "[\n",
    "    'did',\n",
    "    'uncle',\n",
    "    'max',\n",
    "    'like',\n",
    "    'jalapeno',\n",
    "    'dip'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the provided `sample_text` to test our final tokenizer.\n",
    "\n",
    "You might decide to go back and adjust your previous steps so that `sample_text`\n",
    "is created to what you want & expect for your final tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = (\n",
    "    'The first time you see The Second Renaissance it may look boring. '\n",
    "    'Look at it at least twice and definitely watch part 2. '\n",
    "    'It will change your view of the matrix. '\n",
    "    'Are the human people the ones who started the war? Is AI a bad thing ?'\n",
    ")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps before breaking things further\n",
    "def tokenize_text(text: str) -> list[str]:\n",
    "    # COMPLETE: Apply normalization, stop word removal & pretokenization steps\n",
    "    # normalizes text - lowercase, no punctuation, only ASCII\n",
    "    text_normalized: str = normalize_text(text)\n",
    "    # split into smaller parts after normalization\n",
    "    pretokenized_text: list[str] = pretokenize_text(text_normalized)\n",
    "    # remove stop words (defined by me)\n",
    "    my_stop_words = [\n",
    "        'of',\n",
    "        'the',\n",
    "        'a',\n",
    "        'an',\n",
    "        'at',\n",
    "    ]\n",
    "    tokens: list[str] = remove_stop_words(\n",
    "        pretokenized_text,\n",
    "        stop_words=my_stop_words,\n",
    "    )\n",
    "    # finalize tokens by removing empty strings\n",
    "    final_tokens = [token for token in tokens if token]\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text(sample_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-dsnd-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
