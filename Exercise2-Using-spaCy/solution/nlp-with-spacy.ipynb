{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spaCy](https://spacy.io/) is one amongst many great industry used NLP tools.\n",
    "\n",
    "spaCy's strength is it's ability to use pretrained models for a variety of NLP\n",
    "tasks. Users can even provide their own labeled language data to train &\n",
    "fine-tune models in spaCy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll use spaCy's already trained pipeline to perform\n",
    "lemmatization on text as well as do NLP tasks like named entity recognition \n",
    "(NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because spaCy has NLP models trained with relevant language data, the first step\n",
    "is to download these models so they can be utilized.\n",
    "\n",
    "The line below uses spaCy to download the \n",
    "[en_core_web_sm](https://spacy.io/models/en#en_core_web_sm) pipeline. This is \n",
    "ideal since it's relatively small to download (at ~12 MB) and is optimized to\n",
    "use the CPU. It also has lemmatizer and NER components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the `spacy` module and load the trained [pipeline](https://spacy.io/usage/spacy-101#pipelines).\n",
    "\n",
    "\n",
    "![](https://spacy.io/images/pipeline.svg)\n",
    "\n",
    "Below is the standard way the pipeline is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Base Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the text below and use the built-in tokenizer to tokenize the text to get\n",
    "a list of all the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    'Dr. Smith graduated from the University of Washington. '\n",
    "    'He later started an analytics firm called Lux, which catered to enterprise customers.'\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the tokens using spaCy's â€“ it only has to be iterated over\n",
    "tokens = nlp.tokenizer(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check you can iterate over the tokens\n",
    "for token in tokens:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    'The first time you see The Second Renaissance it may look boring. '\n",
    "    'Look at it at least twice and definitely watch part 2. '\n",
    "    'It will change your view of the Matrix. '\n",
    "    'Are the human people the ones who started the war? Is AI a bad thing ?'\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use spaCy to compare the tokens before and after lemmatization\n",
    "doc = nlp(text)\n",
    "diffs = []\n",
    "for token in doc:\n",
    "    orig = token.text \n",
    "    lemma = token.lemma_\n",
    "    if orig != lemma:\n",
    "        print(orig, lemma)\n",
    "        diff = (orig, lemma)\n",
    "        diffs.append(diff)\n",
    "\n",
    "print(len(diffs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now closer at the the tokens, specifically at the parts of speech for each\n",
    "token, according to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    'Dr. Smith graduated from the University of Washington. '\n",
    "    'He later started an analytics firm called Lux, which catered to enterprise customers.'\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_groupings: dict[str, list[str]] = {}\n",
    "# TODO: Group the tokens by parts of speech\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    pos_tag = token.pos_\n",
    "    if pos_tag not in pos_groupings:\n",
    "        pos_groupings[pos_tag] = list()\n",
    "    pos_groupings[pos_tag].append(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos_tag, tokens in pos_groupings.items():\n",
    "    print(pos_tag)\n",
    "    print('\\t', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First determine what are the labels used by the model we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: List the different entity labels used by the spaCy pipeline being used\n",
    "nlp.get_pipe(\"ner\").labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, take the following text and list out all the tokens that have an\n",
    "associated entity label.\n",
    "Also list out the type of entity it is according to the\n",
    "pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    'The first time you see The Second Renaissance it may look boring. '\n",
    "    'Look at it at least twice and definitely watch part 2. '\n",
    "    'It will change your view of the Matrix. '\n",
    "    'Are the human people the ones who started the war? Is AI a bad thing ?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: List only the tokens that are entities along with their labels\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc.ents:\n",
    "    text = token.text\n",
    "    entity = token.label_\n",
    "    print(f'Text: {text}\\n\\t Entity: {entity}')\n",
    "\n",
    "# Alternative\n",
    "print('\\n', 10*'-', 'ALTERNATIVE', '-'*10)\n",
    "for token in doc:\n",
    "    text = token.text\n",
    "    entity = token.ent_type_\n",
    "    if entity:\n",
    "        print(f'Text: {text}\\n\\t Entity: {entity}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-dsnd-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
